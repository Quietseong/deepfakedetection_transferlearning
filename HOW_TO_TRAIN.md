# ğŸš€ Transfer Learning í•™ìŠµ ê°€ì´ë“œ

FSFM ëª¨ë¸ì„ ì‚¬ìš©í•œ ë”¥í˜ì´í¬ íƒì§€ ì „ì´í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

---

## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… 1. í™˜ê²½ í™•ì¸
```bash
# PyTorchì™€ CUDA í™•ì¸
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

### âœ… 2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### âœ… 3. ë°ì´í„° ë¶„í•  (ì²˜ìŒ í•œ ë²ˆë§Œ ì‹¤í–‰)
ê²€ì¦ ë°ì´í„°ê°€ ë¹„ì–´ìˆë‹¤ë©´, í•™ìŠµ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„í• í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
python scripts/split_train_val.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
- í•™ìŠµ ë°ì´í„°ì˜ **20%**ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ì´ë™
- Realê³¼ Fakeë¥¼ ê°ê° ë¹„ìœ¨ì— ë§ì¶° ë¶„í• 
- ëœë¤ ì‹œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ ë¶„í• 

### âœ… 4. ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
```bash
python scripts/test_dataset.py
```

ì˜ˆìƒ ì¶œë ¥:
```
[í•™ìŠµ ë°ì´í„°ì…‹]
âœ“ ì´ ìƒ˜í”Œ ìˆ˜: 12926
âœ“ ìƒ˜í”Œ êµ¬ì¡°:
  - image shape: torch.Size([3, 224, 224])
  - label: 0
  - filename: real_0.jpg
  - is_video: False

[ê²€ì¦ ë°ì´í„°ì…‹]
âœ“ ì´ ìƒ˜í”Œ ìˆ˜: 3232
...
```

### âœ… 5. ëª¨ë¸ í…ŒìŠ¤íŠ¸
```bash
python scripts/test_model.py
```

---

## ğŸ¯ í•™ìŠµ ì „ëµ ì„ íƒ

FSFM ì „ì´í•™ìŠµì€ 3ê°€ì§€ ì „ëµì„ ì§€ì›í•©ë‹ˆë‹¤:

### ì „ëµ 1: Feature Extractor (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©) âš¡
**íŠ¹ì§•:**
- ì‚¬ì „í•™ìŠµëœ ë°±ë³¸ì„ **ì™„ì „íˆ ê³ ì •**
- ë¶„ë¥˜ í—¤ë“œë§Œ í•™ìŠµ
- ë¹ ë¥¸ í•™ìŠµ ì†ë„ (ë©”ëª¨ë¦¬ ì ê²Œ ì‚¬ìš©)
- ì ì€ ë°ì´í„°ì…‹ì— ì í•©

**ì‚¬ìš© ì‹œê¸°:**
- ë¹ ë¥´ê²Œ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ì„ í™•ì¸í•˜ê³  ì‹¶ì„ ë•Œ
- GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ
- ë°ì´í„°ì…‹ì´ ë§¤ìš° ì‘ì„ ë•Œ (< 1000 ìƒ˜í”Œ)

**ì‹¤í–‰ ë°©ë²•:**
```bash
python train.py \
  --config configs/config.yaml \
  --strategy feature_extractor
```

### ì „ëµ 2: Fine-tuning (ê¶Œì¥) ğŸ¯
**íŠ¹ì§•:**
- ì „ì²´ ëª¨ë¸ì„ **í•¨ê»˜ í•™ìŠµ**
- ìµœê³ ì˜ ì„±ëŠ¥
- ì¤‘ê°„ í•™ìŠµ ì‹œê°„
- ì¤‘ê°„ í¬ê¸° ì´ìƒì˜ ë°ì´í„°ì…‹ì— ì í•©

**ì‚¬ìš© ì‹œê¸°:**
- **ìµœê³  ì„±ëŠ¥**ì´ í•„ìš”í•  ë•Œ
- ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œ (> 5000 ìƒ˜í”Œ)
- GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•  ë•Œ

**ì‹¤í–‰ ë°©ë²•:**
```bash
python train.py \
  --config configs/config.yaml \
  --strategy fine_tuning
```

### ì „ëµ 3: PEFT-LoRA (íš¨ìœ¨ì  í•™ìŠµ) ğŸ”§
**íŠ¹ì§•:**
- LoRA ì–´ëŒ‘í„°ë§Œ í•™ìŠµ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- ë¹ ë¥¸ í•™ìŠµ ì†ë„
- Fine-tuningê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥

**ì‚¬ìš© ì‹œê¸°:**
- ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³  ì‹¶ì„ ë•Œ
- ì—¬ëŸ¬ ì‹¤í—˜ì„ ë¹ ë¥´ê²Œ ëŒë¦¬ê³  ì‹¶ì„ ë•Œ
- ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‹œë„í•˜ê³  ì‹¶ì„ ë•Œ

**ì‹¤í–‰ ë°©ë²•:**
```bash
python train.py \
  --config configs/config.yaml \
  --strategy peft_lora
```

---

## ğŸ“Š í•™ìŠµ ì‹œì‘í•˜ê¸°

### ê¸°ë³¸ í•™ìŠµ (Fine-tuning, ê¶Œì¥)
```bash
python train.py \
  --config configs/config.yaml \
  --strategy fine_tuning
```

### ì¶”ê°€ ì˜µì…˜ê³¼ í•¨ê»˜ ì‹¤í–‰
```bash
python train.py \
  --config configs/config.yaml \
  --strategy fine_tuning \
  --batch_size 16 \
  --epochs 30 \
  --lr 5e-5
```

### ëª¨ë¸ ì„ íƒ
`configs/config.yaml`ì—ì„œ ëª¨ë¸ íƒ€ì…ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
model:
  type: "vit_base_patch16"  # ì˜µì…˜: vit_small_patch16, vit_base_patch16, vit_large_patch16
  pretrained_checkpoint: "/workspace/transfer_learning_fsfm/src/models/fsfm/checkpoint/vit_base_patch16/checkpoint-min_val_loss.pth"
  norm_file: "/workspace/transfer_learning_fsfm/src/models/fsfm/checkpoint/vit_base_patch16/pretrain_ds_mean_std.txt"
```

**ëª¨ë¸ í¬ê¸° ë¹„êµ:**
- `vit_small_patch16`: ì‘ê³  ë¹ ë¦„ (~22M params)
- `vit_base_patch16`: ì¤‘ê°„ í¬ê¸°, ê· í˜• ì¡í˜ (~86M params) â­ **ê¶Œì¥**
- `vit_large_patch16`: í¬ê³  ëŠë¦¼, ìµœê³  ì„±ëŠ¥ (~304M params)

---

## ğŸ“ˆ í•™ìŠµ ëª¨ë‹ˆí„°ë§

### í„°ë¯¸ë„ ì¶œë ¥
í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤:
```
Epoch 1/20
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 404/404 [03:24<00:00,  1.97it/s, loss=0.234, acc=0.923]
Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [00:42<00:00,  2.38it/s, loss=0.189, acc=0.941]

Epoch 1 ì™„ë£Œ - Train Loss: 0.2340 | Train Acc: 0.9230 | Val Loss: 0.1890 | Val Acc: 0.9410
âœ“ Best model saved!
```

### TensorBoard (ì„ íƒì‚¬í•­)
```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
tensorboard --logdir logs/

# ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
# http://localhost:6006
```

### ì €ì¥ëœ íŒŒì¼ í™•ì¸
```bash
ls -lh checkpoints/
# best_model.pth - ìµœê³  ì„±ëŠ¥ ëª¨ë¸
# last_model.pth - ë§ˆì§€ë§‰ ì—í¬í¬ ëª¨ë¸
# checkpoint_epoch_5.pth - 5 ì—í¬í¬ë§ˆë‹¤ ì €ì¥
```

---

## ğŸ› ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### í•™ìŠµë¥  ì¡°ì •
```yaml
training:
  optimizer:
    lr: 1e-4  # ê¸°ë³¸ê°’
    # Fine-tuning: 1e-5 ~ 1e-4
    # Feature Extractor: 1e-4 ~ 1e-3
    # PEFT-LoRA: 1e-4 ~ 3e-4
```

### ë°°ì¹˜ í¬ê¸° ì¡°ì •
```yaml
training:
  batch_size: 32  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
  # 16: 12GB GPU
  # 32: 24GB GPU
  # 64: 40GB+ GPU
```

### ë°ì´í„° ì¦ê°• ì„¤ì •
```yaml
training:
  augmentation:
    enabled: true
    horizontal_flip: 0.5
    rotation: 10
    color_jitter:
      brightness: 0.2
      contrast: 0.2
```

---

## ğŸ“ í•™ìŠµ íŒ

### 1. ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•œ ì„¤ì •
ì‘ì€ ì—í¬í¬ë¡œ ì‹œì‘í•˜ì—¬ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸:
```bash
python train.py \
  --config configs/config.yaml \
  --strategy feature_extractor \
  --epochs 5 \
  --batch_size 64
```

### 2. ì˜¤ë²„í”¼íŒ… ë°©ì§€
- Label smoothing í™œì„±í™”
- Weight decay ì¦ê°€
- Dropout ì¦ê°€
- ë°ì´í„° ì¦ê°• ê°•í™”

```yaml
training:
  optimizer:
    weight_decay: 0.1  # ê¸°ë³¸ 0.05
  loss:
    label_smoothing: 0.1
```

### 3. í•™ìŠµì´ ë¶ˆì•ˆì •í•  ë•Œ
- í•™ìŠµë¥  ê°ì†Œ
- Warmup ì—í¬í¬ ì¦ê°€
- Gradient clipping ì¶”ê°€

```yaml
training:
  optimizer:
    lr: 5e-5  # ë‚®ì¶¤
  scheduler:
    warmup_epochs: 3  # ì¦ê°€
```

### 4. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
```yaml
training:
  batch_size: 16  # ì¤„ì´ê¸°
  mixed_precision: true  # í™œì„±í™”
```

ë˜ëŠ”:
```bash
# Gradient accumulation ì‚¬ìš©
python train.py \
  --config configs/config.yaml \
  --batch_size 8 \
  --gradient_accumulation_steps 4
```

---

## ğŸ” í•™ìŠµ í›„ í‰ê°€

### ë² ìŠ¤íŠ¸ ëª¨ë¸ë¡œ ì¶”ë¡ 
```bash
python inference.py \
  --config configs/config.yaml \
  --checkpoint checkpoints/best_model.pth \
  --input_dir data/test \
  --output submission.csv
```

### ì²´í¬í¬ì¸íŠ¸ ë¹„êµ
ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ì˜ ì„±ëŠ¥ì„ ë¹„êµ:
```bash
for ckpt in checkpoints/*.pth; do
    echo "Evaluating $ckpt"
    python inference.py \
      --checkpoint $ckpt \
      --input_dir data/val \
      --output results_$(basename $ckpt .pth).csv
done
```

---

## âš ï¸ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: "ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
**ì›ì¸:** ë°ì´í„° ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆê±°ë‚˜ í´ë”ê°€ ë¹„ì–´ìˆìŒ

**í•´ê²°:**
```bash
# ë°ì´í„° êµ¬ì¡° í™•ì¸
ls -la data/train/
ls -la data/val/

# ê²€ì¦ ë°ì´í„°ê°€ ë¹„ì–´ìˆë‹¤ë©´
python scripts/split_train_val.py
```

### ë¬¸ì œ 2: CUDA Out of Memory
**í•´ê²°:**
- ë°°ì¹˜ í¬ê¸° ê°ì†Œ: `batch_size: 8` ë˜ëŠ” `16`
- Mixed precision í™œì„±í™”: `mixed_precision: true`
- ì‘ì€ ëª¨ë¸ ì‚¬ìš©: `vit_small_patch16`
- Num workers ê°ì†Œ: `num_workers: 2`

### ë¬¸ì œ 3: í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ (lossê°€ ì•ˆ ë–¨ì–´ì§)
**ì›ì¸:** í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ê±°ë‚˜ ë†’ìŒ

**í•´ê²°:**
```bash
# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ í™•ì¸
python train.py --config configs/config.yaml --strategy fine_tuning --lr 1e-4
python train.py --config configs/config.yaml --strategy fine_tuning --lr 5e-5
python train.py --config configs/config.yaml --strategy fine_tuning --lr 3e-4
```

### ë¬¸ì œ 4: ê²€ì¦ ì •í™•ë„ê°€ í•™ìŠµ ì •í™•ë„ë³´ë‹¤ ë†’ìŒ
**ì›ì¸:** ë°ì´í„° ì¦ê°•ì´ ê°•í•˜ê±°ë‚˜ ì •ê·œí™”ê°€ ê°•í•¨

**í•´ê²°:**
- ë°ì´í„° ì¦ê°• ì•½í™”
- Weight decay ê°ì†Œ
- Dropout ê°ì†Œ

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

í•™ìŠµ ì‹œì‘ ì „ í™•ì¸:

- [ ] ë°ì´í„°ê°€ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë¡œ ì¤€ë¹„ë˜ì–´ ìˆëŠ”ê°€? (`Real/`, `Fake/`)
- [ ] ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ”ê°€? (ì—†ìœ¼ë©´ `split_train_val.py` ì‹¤í–‰)
- [ ] `configs/config.yaml`ì—ì„œ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ê°€?
- [ ] ì‚¬ì „í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ê°€ ì¡´ì¬í•˜ëŠ”ê°€?
- [ ] GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œê°€?
- [ ] í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ê°€?

---

## ğŸ¯ ê¶Œì¥ ì›Œí¬í”Œë¡œìš°

### ì²« ì‹¤í–‰ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
```bash
# 1. ë°ì´í„° ë¶„í• 
python scripts/split_train_val.py

# 2. ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
python scripts/test_dataset.py

# 3. Feature Extractorë¡œ ë¹ ë¥¸ ë² ì´ìŠ¤ë¼ì¸ í™•ì¸ (5 ì—í¬í¬)
python train.py --config configs/config.yaml --strategy feature_extractor --epochs 5

# 4. ê²°ê³¼ í™•ì¸
ls -lh checkpoints/
```

### ë³¸ê²©ì ì¸ í•™ìŠµ
```bash
# Fine-tuningìœ¼ë¡œ 20 ì—í¬í¬ í•™ìŠµ
python train.py --config configs/config.yaml --strategy fine_tuning --epochs 20
```

### ìµœì í™”
```bash
# ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í—˜
python train.py --config configs/config.yaml --strategy fine_tuning --lr 5e-5 --epochs 30
python train.py --config configs/config.yaml --strategy peft_lora --lr 1e-4 --epochs 25
```

---

## ğŸ“š ì¶”ê°€ ìë£Œ

- **README.md**: í”„ë¡œì íŠ¸ ì „ì²´ ê°œìš”
- **IMPLEMENTATION_GUIDE.md**: ì½”ë“œ êµ¬í˜„ ìƒì„¸ ê°€ì´ë“œ
- **PROJECT_SUMMARY.md**: í”„ë¡œì íŠ¸ ìš”ì•½ ë° êµ¬ì¡°

---

**Good Luck! ğŸš€**

