"""
ì „ì´í•™ìŠµ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

FSFM ëª¨ë¸ì„ ë”¥í˜ì´í¬ íƒì§€ íƒœìŠ¤í¬ì— ì „ì´í•™ìŠµì‹œí‚µë‹ˆë‹¤.
"""

import os
import sys
import argparse
import random
from pathlib import Path
from typing import Dict, Tuple
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent))
from src.data import DeepfakeDataset, collate_fn
from src.models import create_model
from src.utils import ConfigLoader, calculate_metrics, print_metrics


def set_seed(seed: int) -> None:
    """
    ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
    
    Args:
        seed: ì‹œë“œ ê°’
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class Trainer:
    """
    ì „ì´í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
    
    Args:
        config: ì„¤ì • ê°ì²´
        model: í•™ìŠµí•  ëª¨ë¸
        train_loader: í•™ìŠµ ë°ì´í„° ë¡œë”
        val_loader: ê²€ì¦ ë°ì´í„° ë¡œë”
        device: í•™ìŠµ ë””ë°”ì´ìŠ¤
    """
    
    def __init__(
        self,
        config: ConfigLoader,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        device: torch.device,
    ):
        self.config = config
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.optimizer = self._create_optimizer()
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.scheduler = self._create_scheduler()
        
        # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •
        self.criterion = self._create_criterion()
        
        # Mixed Precision Training
        self.use_amp = config.get("training.mixed_precision", False)
        self.scaler = GradScaler() if self.use_amp else None
        
        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
        self.checkpoint_dir = Path(config.get("training.checkpoint_dir", "./checkpoints"))
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # Early Stopping
        self.early_stopping_patience = config.get("training.early_stopping.patience", 7)
        self.early_stopping_counter = 0
        self.best_val_f1 = 0.0
    
    def _create_optimizer(self) -> optim.Optimizer:
        """
        ì˜µí‹°ë§ˆì´ì € ìƒì„±
        
        Returns:
            ì˜µí‹°ë§ˆì´ì €
        """
        optimizer_type = self.config.get("training.optimizer.type", "adamw").lower()
        lr = float(self.config.get("training.optimizer.lr", 1e-4))
        weight_decay = float(self.config.get("training.optimizer.weight_decay", 0.05))
        
        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ì „ë‹¬
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        if optimizer_type == "adamw":
            betas = self.config.get("training.optimizer.betas", [0.9, 0.999])
            optimizer = optim.AdamW(
                trainable_params,
                lr=lr,
                betas=betas,
                weight_decay=weight_decay,
            )
        elif optimizer_type == "sgd":
            momentum = self.config.get("training.optimizer.momentum", 0.9)
            optimizer = optim.SGD(
                trainable_params,
                lr=lr,
                momentum=momentum,
                weight_decay=weight_decay,
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {optimizer_type}")
        
        print(f"\nì˜µí‹°ë§ˆì´ì €: {optimizer_type.upper()}")
        print(f"  - Learning Rate: {lr}")
        print(f"  - Weight Decay: {weight_decay}")
        
        return optimizer
    
    def _create_scheduler(self) -> optim.lr_scheduler._LRScheduler:
        """
        í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
        
        Returns:
            í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        """
        scheduler_type = self.config.get("training.scheduler.type", "cosine").lower()
        
        if scheduler_type == "cosine":
            epochs = int(self.config.get("training.epochs", 20))
            min_lr = float(self.config.get("training.scheduler.min_lr", 1e-6))
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=epochs,
                eta_min=min_lr,
            )
            print(f"í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: CosineAnnealingLR")
        
        elif scheduler_type == "step":
            step_size = int(self.config.get("training.scheduler.step_size", 5))
            gamma = float(self.config.get("training.scheduler.gamma", 0.1))
            scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=step_size,
                gamma=gamma,
            )
            print(f"í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: StepLR")
        
        elif scheduler_type == "plateau":
            patience = int(self.config.get("training.scheduler.patience", 3))
            factor = float(self.config.get("training.scheduler.factor", 0.5))
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode="max",
                factor=factor,
                patience=patience,
                verbose=True,
            )
            print(f"í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: ReduceLROnPlateau")
        
        else:
            scheduler = None
            print("í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬: ì—†ìŒ")
        
        return scheduler
    
    def _create_criterion(self) -> nn.Module:
        """
        ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
        
        Returns:
            ì†ì‹¤ í•¨ìˆ˜
        """
        loss_type = self.config.get("training.loss.type", "cross_entropy").lower()
        
        if loss_type == "cross_entropy":
            label_smoothing = float(self.config.get("training.loss.label_smoothing", 0.0))
            criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
            print(f"ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss (label_smoothing={label_smoothing})")
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ì‹¤ í•¨ìˆ˜: {loss_type}")
        
        return criterion
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """
        í•œ ì—í­ í•™ìŠµ
        
        Args:
            epoch: í˜„ì¬ ì—í­
            
        Returns:
            í•™ìŠµ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        self.model.train()
        
        running_loss = 0.0
        all_preds = []
        all_labels = []
        
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {epoch} [Train]")
        
        for batch_idx, batch in enumerate(progress_bar):
            images = batch["image"].to(self.device)
            labels = batch["label"].to(self.device)
            
            # Forward pass (Mixed Precision)
            if self.use_amp:
                with autocast():
                    outputs = self.model(images)
                    loss = self.criterion(outputs, labels)
                
                # Backward pass
                self.optimizer.zero_grad()
                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                # Backward pass
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
            
            # ì˜ˆì¸¡ ë° í†µê³„
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            running_loss += loss.item()
            
            # Progress bar ì—…ë°ì´íŠ¸
            if batch_idx % self.config.get("logging.log_interval", 10) == 0:
                progress_bar.set_postfix({
                    "loss": f"{loss.item():.4f}",
                    "lr": f"{self.optimizer.param_groups[0]['lr']:.6f}",
                })
        
        # ì—í­ í‰ê·  ì†ì‹¤
        avg_loss = running_loss / len(self.train_loader)
        
        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        metrics = calculate_metrics(
            np.array(all_labels),
            np.array(all_preds),
            detailed=True,
        )
        metrics["loss"] = avg_loss
        
        return metrics
    
    @torch.no_grad()
    def validate(self, epoch: int) -> Dict[str, float]:
        """
        ê²€ì¦ ìˆ˜í–‰
        
        Args:
            epoch: í˜„ì¬ ì—í­
            
        Returns:
            ê²€ì¦ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        self.model.eval()
        
        running_loss = 0.0
        all_preds = []
        all_labels = []
        
        progress_bar = tqdm(self.val_loader, desc=f"Epoch {epoch} [Val]")
        
        for batch in progress_bar:
            images = batch["image"].to(self.device)
            labels = batch["label"].to(self.device)
            
            # Forward pass
            if self.use_amp:
                with autocast():
                    outputs = self.model(images)
                    loss = self.criterion(outputs, labels)
            else:
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
            
            # ì˜ˆì¸¡ ë° í†µê³„
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            running_loss += loss.item()
            
            # Progress bar ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
        
        # í‰ê·  ì†ì‹¤
        avg_loss = running_loss / len(self.val_loader)
        
        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        metrics = calculate_metrics(
            np.array(all_labels),
            np.array(all_preds),
            detailed=True,
        )
        metrics["loss"] = avg_loss
        
        return metrics
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float], is_best: bool = False) -> None:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        
        Args:
            epoch: í˜„ì¬ ì—í­
            metrics: í‰ê°€ ì§€í‘œ
            is_best: ìµœê³  ì„±ëŠ¥ ì—¬ë¶€
        """
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "metrics": metrics,
        }
        
        if self.scheduler is not None:
            checkpoint["scheduler_state_dict"] = self.scheduler.state_dict()
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if is_best:
            best_path = self.checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            print(f"  âœ“ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {best_path}")
        
        # ì£¼ê¸°ì  ì €ì¥
        save_every_n = self.config.get("training.save_every_n_epochs", 5)
        if epoch % save_every_n == 0:
            epoch_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch}.pth"
            torch.save(checkpoint, epoch_path)
            print(f"  âœ“ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {epoch_path}")
    
    def train(self) -> None:
        """
        ì „ì²´ í•™ìŠµ ë£¨í”„
        """
        epochs = self.config.get("training.epochs", 20)
        
        print("\n" + "="*60)
        print("í•™ìŠµ ì‹œì‘")
        print("="*60)
        
        for epoch in range(1, epochs + 1):
            print(f"\n{'='*60}")
            print(f"Epoch {epoch}/{epochs}")
            print(f"{'='*60}")
            
            # í•™ìŠµ
            train_metrics = self.train_epoch(epoch)
            print(f"\n[Train] Loss: {train_metrics['loss']:.4f} | "
                  f"Acc: {train_metrics['accuracy']:.4f} | "
                  f"Macro F1: {train_metrics['macro_f1']:.4f} | "
                  f"F1 (Fake): {train_metrics['f1_fake']:.4f} | "
                  f"F1 (Real): {train_metrics['f1_real']:.4f}")
            
            # ê²€ì¦
            val_metrics = self.validate(epoch)
            print(f"[Val]   Loss: {val_metrics['loss']:.4f} | "
                  f"Acc: {val_metrics['accuracy']:.4f} | "
                  f"Macro F1: {val_metrics['macro_f1']:.4f} â­ | "
                  f"F1 (Fake): {val_metrics['f1_fake']:.4f} | "
                  f"F1 (Real): {val_metrics['f1_real']:.4f}")
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics['macro_f1'])
                else:
                    self.scheduler.step()
            
            # ìµœê³  ì„±ëŠ¥ ì²´í¬
            current_val_f1 = val_metrics['macro_f1']
            is_best = current_val_f1 > self.best_val_f1
            
            if is_best:
                self.best_val_f1 = current_val_f1
                self.early_stopping_counter = 0
                print(f"\n  ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! Macro F1: {self.best_val_f1:.4f}")
            else:
                self.early_stopping_counter += 1
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            self.save_checkpoint(epoch, val_metrics, is_best=is_best)
            
            # Early Stopping
            if (self.config.get("training.early_stopping.enabled", False) 
                and self.early_stopping_counter >= self.early_stopping_patience):
                print(f"\nâš ï¸  Early Stopping: {self.early_stopping_patience} ì—í­ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                break
        
        print("\n" + "="*60)
        print("í•™ìŠµ ì™„ë£Œ!")
        print(f"ìµœê³  Validation Macro F1: {self.best_val_f1:.4f}")
        print("="*60)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="FSFM ì „ì´í•™ìŠµ")
    parser.add_argument(
        "--config",
        type=str,
        default="configs/config.yaml",
        help="ì„¤ì • íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--strategy",
        type=str,
        default=None,
        help="ì „ì´í•™ìŠµ ì „ëµ (feature_extractor, fine_tuning, peft_lora)",
    )
    args, _ = parser.parse_known_args()
    
    # ì„¤ì • ë¡œë“œ
    config = ConfigLoader(args.config)
    
    # ì „ëµ ì˜¤ë²„ë¼ì´ë“œ
    if args.strategy is not None:
        config.config["transfer_learning"]["strategy"] = args.strategy
    
    # ì‹œë“œ ì„¤ì •
    seed = config.get("project.seed", 42)
    set_seed(seed)
    print(f"ì‹œë“œ ì„¤ì •: {seed}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device(
        config.get("project.device", "cuda") 
        if torch.cuda.is_available() 
        else "cpu"
    )
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("\në°ì´í„° ë¡œë“œ ì¤‘...")
    train_dataset = DeepfakeDataset(
        data_dir=config.get("data.train_path"),
        is_training=True,
        num_frames=config.get("data.num_frames", 10),
        image_size=config.get("data.image_size", 224),
        mean=config.get("data.mean"),
        std=config.get("data.std"),
    )
    
    val_dataset = DeepfakeDataset(
        data_dir=config.get("data.val_path"),
        is_training=False,
        num_frames=config.get("data.num_frames", 10),
        image_size=config.get("data.image_size", 224),
        mean=config.get("data.mean"),
        std=config.get("data.std"),
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.get("training.batch_size", 32),
        shuffle=True,
        num_workers=config.get("training.num_workers", 4),
        pin_memory=config.get("training.pin_memory", True),
        collate_fn=collate_fn,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.get("training.batch_size", 32),
        shuffle=False,
        num_workers=config.get("training.num_workers", 4),
        pin_memory=config.get("training.pin_memory", True),
        collate_fn=collate_fn,
    )
    
    print(f"  - í•™ìŠµ ìƒ˜í”Œ: {len(train_dataset)}")
    print(f"  - ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset)}")
    
    # ëª¨ë¸ ìƒì„±
    print("\nëª¨ë¸ ìƒì„± ì¤‘...")
    strategy = config.get("transfer_learning.strategy", "fine_tuning")
    
    if strategy == "peft_lora":
        model = create_model(
            model_type=config.get("model.type"),
            num_classes=config.get("model.num_classes", 2),
            pretrained_path=config.get("model.pretrained_checkpoint"),
            drop_path_rate=config.get("model.drop_path_rate", 0.1),
            global_pool=config.get("model.global_pool", True),
            strategy=strategy,
            lora_r=config.get("transfer_learning.peft_lora.r", 16),
            lora_alpha=config.get("transfer_learning.peft_lora.lora_alpha", 32),
            lora_dropout=config.get("transfer_learning.peft_lora.lora_dropout", 0.1),
            target_modules=config.get("transfer_learning.peft_lora.target_modules", ["qkv"]),
        )
    else:
        freeze_layers = None
        if strategy == "fine_tuning":
            freeze_layers = config.get("transfer_learning.fine_tuning.freeze_layers", [])
        
        model = create_model(
            model_type=config.get("model.type"),
            num_classes=config.get("model.num_classes", 2),
            pretrained_path=config.get("model.pretrained_checkpoint"),
            drop_path_rate=config.get("model.drop_path_rate", 0.1),
            global_pool=config.get("model.global_pool", True),
            strategy=strategy,
            freeze_layers=freeze_layers,
        )
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
    trainer = Trainer(
        config=config,
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
    )
    
    trainer.train()


if __name__ == "__main__":
    main()


