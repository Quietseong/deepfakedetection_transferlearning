# FSFM ì „ì´í•™ìŠµ í”„ë¡œì íŠ¸

ë”¥í˜ì´í¬ íƒì§€ë¥¼ ìœ„í•œ FSFM(Few-Shot Face Manipulation) ëª¨ë¸ ì „ì´í•™ìŠµ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” ì‚¬ì „í•™ìŠµëœ FSFM ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ë° ë™ì˜ìƒì˜ ë”¥í˜ì´í¬ ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” ë¶„ë¥˜ ëª¨ë¸ì„ ê°œë°œí•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- âœ… **ë‹¤ì–‘í•œ ì „ì´í•™ìŠµ ì „ëµ ì§€ì›**
  - Feature Extractor: ë°±ë³¸ ê³ ì •, í—¤ë“œë§Œ í•™ìŠµ
  - Fine-tuning: ì „ì²´/ë¶€ë¶„ ë¯¸ì„¸ì¡°ì •
  - PEFT-LoRA: íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  ë¯¸ì„¸ì¡°ì •
- âœ… **ì´ë¯¸ì§€ ë° ë™ì˜ìƒ ì²˜ë¦¬**
- âœ… **Macro F1-score ê¸°ë°˜ í‰ê°€**
- âœ… **Mixed Precision Training ì§€ì›**
- âœ… **Early Stopping ë° ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬**

---

## ğŸ—‚ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
transfer_learning_fsfm/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # ì„¤ì • íŒŒì¼
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ dataset.py           # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ transfer_model.py    # ì „ì´í•™ìŠµ ëª¨ë¸
â”‚   â”‚   â””â”€â”€ fsfm/                # FSFM ëª¨ë¸ (ë³µì‚¬ë¨)
â”‚   â”‚       â”œâ”€â”€ models_vit.py
â”‚   â”‚       â””â”€â”€ checkpoint/
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ metrics.py           # í‰ê°€ ì§€í‘œ
â”‚       â””â”€â”€ config_loader.py     # ì„¤ì • ë¡œë”
â”œâ”€â”€ checkpoints/                 # í•™ìŠµ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ logs/                        # í•™ìŠµ ë¡œê·¸
â”œâ”€â”€ train.py                     # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ inference.py                 # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt             # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â””â”€â”€ README.md                    # ì´ ë¬¸ì„œ
```

---

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •

#### Python ë²„ì „
- Python 3.10 ì´ìƒ ê¶Œì¥

#### íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# CUDA 12.6 í™˜ê²½
pip install -U torch==2.7.1 torchvision==0.22.1 --index-url https://download.pytorch.org/whl/cu126

# ê¸°íƒ€ íŒ¨í‚¤ì§€
pip install -r requirements.txt
```

#### PEFT-LoRA ì‚¬ìš© ì‹œ (ì„ íƒì‚¬í•­)
```bash
pip install peft transformers accelerate
```

### 2. ë°ì´í„° ì¤€ë¹„

ë°ì´í„°ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ real/       # ì§„ì§œ ì´ë¯¸ì§€/ë™ì˜ìƒ (ë¼ë²¨ 0)
â”‚   â”‚   â”œâ”€â”€ real_001.jpg
â”‚   â”‚   â”œâ”€â”€ real_002.mp4
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ fake/       # ê°€ì§œ ì´ë¯¸ì§€/ë™ì˜ìƒ (ë¼ë²¨ 1)
â”‚       â”œâ”€â”€ fake_001.jpg
â”‚       â”œâ”€â”€ fake_002.mp4
â”‚       â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ real/
â”‚   â””â”€â”€ fake/
â””â”€â”€ test/           # ì¶”ë¡ ìš© (ë¼ë²¨ ì—†ìŒ)
    â”œâ”€â”€ sample_001.jpg
    â”œâ”€â”€ sample_002.mp4
    â””â”€â”€ ...
```

### 3. ì„¤ì • íŒŒì¼ ìˆ˜ì •

`configs/config.yaml` íŒŒì¼ì„ ì—´ì–´ ë‹¤ìŒ í•­ëª©ì„ ìˆ˜ì •í•˜ì„¸ìš”:

```yaml
# ë°ì´í„° ê²½ë¡œ
data:
  train_path: "/path/to/your/data/train"
  val_path: "/path/to/your/data/val"
  inference_path: "/path/to/your/data/test"

# ëª¨ë¸ ì„¤ì •
model:
  pretrained_checkpoint: "/path/to/fsfm/checkpoint/vit_base_patch16/checkpoint-min_val_loss.pth"

# ì „ì´í•™ìŠµ ì „ëµ ì„ íƒ
transfer_learning:
  strategy: "fine_tuning"  # feature_extractor, fine_tuning, peft_lora
```

---

## ğŸ¯ ì „ì´í•™ìŠµ ì „ëµ

### 1. Feature Extractor

ë°±ë³¸ì„ ê³ ì •í•˜ê³  ë¶„ë¥˜ í—¤ë“œë§Œ í•™ìŠµí•©ë‹ˆë‹¤.

**ì¥ì :**
- ë¹ ë¥¸ í•™ìŠµ
- ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
- ì‘ì€ ë°ì´í„°ì…‹ì— ì í•©

**ì‚¬ìš©ë²•:**
```yaml
# configs/config.yaml
transfer_learning:
  strategy: "feature_extractor"
```

### 2. Fine-tuning

ì „ì²´ ë˜ëŠ” ì¼ë¶€ ë ˆì´ì–´ë¥¼ ë¯¸ì„¸ì¡°ì •í•©ë‹ˆë‹¤.

**ì¥ì :**
- ë†’ì€ ì„±ëŠ¥
- ìœ ì—°í•œ ì¡°ì •

**ì‚¬ìš©ë²•:**
```yaml
# ì „ì²´ ë¯¸ì„¸ì¡°ì •
transfer_learning:
  strategy: "fine_tuning"
  fine_tuning:
    freeze_layers: []  # ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ í•™ìŠµ

# ë¶€ë¶„ ë¯¸ì„¸ì¡°ì • (ì´ˆê¸° 4ê°œ ë¸”ë¡ ê³ ì •)
transfer_learning:
  strategy: "fine_tuning"
  fine_tuning:
    freeze_layers: ["blocks.0", "blocks.1", "blocks.2", "blocks.3"]
```

### 3. PEFT-LoRA

LoRA ì–´ëŒ‘í„°ë§Œ í•™ìŠµí•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¯¸ì„¸ì¡°ì •í•©ë‹ˆë‹¤.

**ì¥ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- ëŒ€ê·œëª¨ ëª¨ë¸ì— ì í•©
- Fine-tuningì— ì¤€í•˜ëŠ” ì„±ëŠ¥

**ì‚¬ìš©ë²•:**
```yaml
transfer_learning:
  strategy: "peft_lora"
  peft_lora:
    r: 16                      # LoRA rank
    lora_alpha: 32             # Scaling factor
    lora_dropout: 0.1
    target_modules: ["qkv"]    # Attention QKVì— ì ìš©
```

---

## ğŸ‹ï¸ í•™ìŠµ

### ê¸°ë³¸ í•™ìŠµ

```bash
python train.py --config configs/config.yaml
```

### ì „ëµ ì§€ì • í•™ìŠµ

```bash
# Feature Extractor
python train.py --config configs/config.yaml --strategy feature_extractor

# Fine-tuning
python train.py --config configs/config.yaml --strategy fine_tuning

# PEFT-LoRA
python train.py --config configs/config.yaml --strategy peft_lora
```

### í•™ìŠµ ëª¨ë‹ˆí„°ë§

í•™ìŠµ ì¤‘ ë‹¤ìŒ ì •ë³´ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤:
- Loss
- Accuracy
- Macro F1-score â­ (ëŒ€íšŒ ì£¼ìš” ì§€í‘œ)
- F1-score (Real / Fake)

ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì€ `checkpoints/best_model.pth`ì— ìë™ ì €ì¥ë©ë‹ˆë‹¤.

---

## ğŸ”® ì¶”ë¡ 

### ê¸°ë³¸ ì¶”ë¡ 

```bash
python inference.py \
  --config configs/config.yaml \
  --checkpoint checkpoints/best_model.pth \
  --data_dir /path/to/test/data \
  --output submission.csv
```

### ì¶œë ¥ í˜•ì‹

`submission.csv`:
```csv
filename,label
sample_001.jpg,0
sample_002.mp4,1
sample_003.jpg,1
...
```

- `filename`: íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
- `label`: ì˜ˆì¸¡ ê²°ê³¼ (Real: 0, Fake: 1)

---

## ğŸ“Š í‰ê°€ ì§€í‘œ

### Macro F1-score

ëŒ€íšŒì˜ ì£¼ìš” í‰ê°€ ì§€í‘œë¡œ, ê° í´ë˜ìŠ¤(Real, Fake)ì˜ F1-score í‰ê· ì…ë‹ˆë‹¤.

```
F1_Real = 2 * TP_Real / (2 * TP_Real + FP_Real + FN_Real)
F1_Fake = 2 * TP_Fake / (2 * TP_Fake + FP_Fake + FN_Fake)

Macro F1 = (F1_Real + F1_Fake) / 2
```

### í´ë˜ìŠ¤ ì •ì˜
- **Positive (ì–‘ì„±)**: Fake (ë¼ë²¨ 1)
- **Negative (ìŒì„±)**: Real (ë¼ë²¨ 0)

---

## âš™ï¸ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

### í•™ìŠµ ì„¤ì •

```yaml
training:
  epochs: 20                # í•™ìŠµ ì—í­
  batch_size: 32            # ë°°ì¹˜ í¬ê¸°
  
  # ì˜µí‹°ë§ˆì´ì €
  optimizer:
    type: "adamw"
    lr: 1e-4                # í•™ìŠµë¥  (ì „ì´í•™ìŠµìš© ë‚®ì€ ê°’)
    weight_decay: 0.05
  
  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
  scheduler:
    type: "cosine"          # cosine, step, plateau
    min_lr: 1e-6
  
  # ì†ì‹¤ í•¨ìˆ˜
  loss:
    type: "cross_entropy"
    label_smoothing: 0.1
  
  # Mixed Precision
  mixed_precision: true
  
  # Early Stopping
  early_stopping:
    enabled: true
    patience: 7
```

### ë°ì´í„° ì¦ê°•

```yaml
training:
  augmentation:
    enabled: true
    horizontal_flip: 0.5
    rotation: 10
    color_jitter:
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    random_erasing: 0.1
```

---

## ğŸ“ ì„±ëŠ¥ í–¥ìƒ íŒ

### 1. ì „ì´í•™ìŠµ ì „ëµ ì„ íƒ
- **ë°ì´í„°ê°€ ì ì„ ë•Œ**: Feature Extractor
- **ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œ**: Fine-tuning
- **GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ**: PEFT-LoRA

### 2. í•™ìŠµë¥  ì¡°ì •
- Feature Extractor: `1e-3` ~ `5e-3`
- Fine-tuning: `1e-4` ~ `5e-4`
- PEFT-LoRA: `1e-4` ~ `1e-3`

### 3. ë°ì´í„° ì¦ê°•
- í•™ìŠµ ë°ì´í„°ê°€ ì ì„ ë•Œ ì¦ê°• ê°•ë„ ë†’ì´ê¸°
- ê³¼ì í•© ë°œìƒ ì‹œ ì¦ê°• í™œì„±í™”

### 4. ë°°ì¹˜ í¬ê¸°
- GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • (32, 64, 128 ë“±)
- ì‘ì€ ë°°ì¹˜ ì‚¬ìš© ì‹œ learning rateë„ ë‚®ì¶”ê¸°

### 5. ë™ì˜ìƒ í”„ë ˆì„ ìˆ˜
- ì„±ëŠ¥: ë§ì„ìˆ˜ë¡ ì¢‹ìŒ (10~32 í”„ë ˆì„)
- ì†ë„: ì ì„ìˆ˜ë¡ ë¹ ë¦„ (5~10 í”„ë ˆì„)

---

## ğŸ› ë¬¸ì œ í•´ê²°

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```yaml
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
training:
  batch_size: 16  # 32 -> 16

# PEFT-LoRA ì‚¬ìš©
transfer_learning:
  strategy: "peft_lora"

# Mixed Precision í™œì„±í™”
training:
  mixed_precision: true
```

### ê³¼ì í•© (Overfitting)
```yaml
# ë°ì´í„° ì¦ê°• ê°•í™”
training:
  augmentation:
    enabled: true
    horizontal_flip: 0.5
    random_erasing: 0.2

# Label Smoothing
training:
  loss:
    label_smoothing: 0.1

# Weight Decay ì¦ê°€
training:
  optimizer:
    weight_decay: 0.1
```

### ê³¼ì†Œì í•© (Underfitting)
```yaml
# í•™ìŠµë¥  ì¦ê°€
training:
  optimizer:
    lr: 5e-4  # 1e-4 -> 5e-4

# ì—í­ ìˆ˜ ì¦ê°€
training:
  epochs: 30

# ë” ë§ì€ ë ˆì´ì–´ í•™ìŠµ
transfer_learning:
  strategy: "fine_tuning"
  fine_tuning:
    freeze_layers: []  # ì „ì²´ ë¯¸ì„¸ì¡°ì •
```

---

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” FSFM ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì›ë³¸ ì½”ë“œëŠ” Attribution-NonCommercial 4.0 International Licenseë¥¼ ë”°ë¦…ë‹ˆë‹¤.

---

## ğŸ™ ê°ì‚¬ì˜ ë§

- FSFM ëª¨ë¸: [ì›ë³¸ ë ˆí¬ì§€í† ë¦¬ ë§í¬]
- timm ë¼ì´ë¸ŒëŸ¬ë¦¬: https://github.com/rwightman/pytorch-image-models

---

## ğŸ“§ ë¬¸ì˜

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”.


